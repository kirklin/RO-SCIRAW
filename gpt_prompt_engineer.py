from openai import OpenAI
from prettytable import PrettyTable
from tqdm import tqdm
import itertools
from tenacity import retry, stop_after_attempt, wait_exponential

# Configuration
OPENAI_API_KEY = "your-api-key-here"  # Enter your OpenAI API key

# Constants
K = 32  # K factor for the ELO rating system
CANDIDATE_MODEL = 'gpt-4o'  # Model for generating candidate prompts
CANDIDATE_MODEL_TEMPERATURE = 0.9  # Temperature parameter for the candidate model
GENERATION_MODEL = 'gpt-4o-mini'  # Model for generating responses
GENERATION_MODEL_TEMPERATURE = 0.8  # Temperature parameter for the generation model
GENERATION_MODEL_MAX_TOKENS = 60  # Maximum number of tokens for the generation model
N_RETRIES = 3  # Number of retries
RANKING_MODEL = 'gpt-4o-mini'  # Model for ranking
RANKING_MODEL_TEMPERATURE = 0.5  # Temperature parameter for the ranking model
NUMBER_OF_PROMPTS = 10  # Number of prompts to generate

client = OpenAI(api_key=OPENAI_API_KEY)

# Prompts
system_gen_system_prompt = """Your task is to generate system prompts for GPT-4 based on a given use case description and some test cases.

You will generate prompts for free-form tasks such as generating landing page headlines, introductory paragraphs, solving math problems, etc.

In your generated prompts, you should describe in simple English how the AI should behave. Include what it will see and what it's allowed to output. Be creative in designing prompts to get the best possible results. The AI knows it's an AI -- you don't need to tell it this.

You will be scored based on how well your prompts perform... but don't cheat! You can't include specific details about the test cases in your prompts. Any prompts that include examples will be disqualified.

Most importantly, only output the prompt. Don't include anything else in your message."""

ranking_system_prompt = """Your task is to rank the quality of two outputs generated by different prompts. These prompts are used to generate responses for a given task.

You will be provided with a task description, a test prompt, and two generated results - one for each system prompt.

Rank the generated results by quality. If generation A is better, respond with 'A'. If generation B is better,  respond with 'B'.

Remember, to be considered 'better', a generation must not only be good but must be noticeably superior to the other.

Also, remember that you are a very harsh critic. Only rank a generation as better if it truly impresses you more.

Only respond with your ranking, don't include anything else. Be fair and unbiased in your judgment."""


def generate_candidate_prompts(description, test_cases, number_of_prompts):
    """Generate candidate prompts"""
    outputs = client.chat.completions.create(
        model=CANDIDATE_MODEL,
        messages=[
            {"role": "system", "content": system_gen_system_prompt},
            {"role": "user",
             "content": f"Here are some test cases: `{test_cases}`\n\nHere's the description of the use case: `{description.strip()}`\n\nOnly respond with your prompt, don't include anything else. Be creative."}
        ],
        temperature=CANDIDATE_MODEL_TEMPERATURE,
        n=number_of_prompts
    )
    prompts = [i.message.content for i in outputs.choices]
    return prompts


def expected_score(r1, r2):
    """Calculate expected score"""
    return 1 / (1 + 10 ** ((r2 - r1) / 400))


def update_elo(r1, r2, score1):
    """Update ELO ratings"""
    e1 = expected_score(r1, r2)
    e2 = expected_score(r2, r1)
    return r1 + K * (score1 - e1), r2 + K * ((1 - score1) - e2)


@retry(stop=stop_after_attempt(N_RETRIES), wait=wait_exponential(multiplier=1, min=4, max=70))
def get_score(description, test_case, pos1, pos2, ranking_model_name, ranking_model_temperature):
    """Get score for generated results"""
    score = client.chat.completions.create(
        model=ranking_model_name,
        messages=[
            {"role": "system", "content": ranking_system_prompt},
            {"role": "user", "content": f"""Task: {description.strip()}
Prompt: {test_case['prompt']}
Generation A: {pos1}
Generation B: {pos2}"""}
        ],
        logit_bias={
            '32': 100,  # 'A' token
            '33': 100,  # 'B' token
        },
        max_tokens=1,
        temperature=ranking_model_temperature,
    ).choices[0].message.content
    return score


@retry(stop=stop_after_attempt(N_RETRIES), wait=wait_exponential(multiplier=1, min=4, max=70))
def get_generation(prompt, test_case):
    """Get generated response"""
    generation = client.chat.completions.create(
        model=GENERATION_MODEL,
        messages=[
            {"role": "system", "content": prompt},
            {"role": "user", "content": f"{test_case['prompt']}"}
        ],
        max_tokens=GENERATION_MODEL_MAX_TOKENS,
        temperature=GENERATION_MODEL_TEMPERATURE,
    ).choices[0].message.content
    return generation


def test_candidate_prompts(test_cases, description, prompts):
    """Test candidate prompts"""
    prompt_ratings = {prompt: 1200 for prompt in prompts}
    total_rounds = len(test_cases) * len(prompts) * (len(prompts) - 1) // 2
    pbar = tqdm(total=total_rounds, ncols=70)

    for prompt1, prompt2 in itertools.combinations(prompts, 2):
        for test_case in test_cases:
            pbar.update()
            generation1 = get_generation(prompt1, test_case)
            generation2 = get_generation(prompt2, test_case)
            score1 = get_score(description, test_case, generation1, generation2, RANKING_MODEL,
                               RANKING_MODEL_TEMPERATURE)
            score2 = get_score(description, test_case, generation2, generation1, RANKING_MODEL,
                               RANKING_MODEL_TEMPERATURE)
            score1 = 1 if score1 == 'A' else 0 if score1 == 'B' else 0.5
            score2 = 1 if score2 == 'B' else 0 if score2 == 'A' else 0.5
            score = (score1 + score2) / 2
            r1, r2 = prompt_ratings[prompt1], prompt_ratings[prompt2]
            r1, r2 = update_elo(r1, r2, score)
            prompt_ratings[prompt1], prompt_ratings[prompt2] = r1, r2

            if score > 0.5:
                print(f"Winner: {prompt1}")
            elif score < 0.5:
                print(f"Winner: {prompt2}")
            else:
                print("Tie")

    pbar.close()
    return prompt_ratings


def generate_optimal_prompt(description, test_cases, number_of_prompts=10):
    """Generate optimal prompt"""
    prompts = generate_candidate_prompts(description, test_cases, number_of_prompts)
    prompt_ratings = test_candidate_prompts(test_cases, description, prompts)

    table = PrettyTable()
    table.field_names = ["Prompt", "Rating"]
    for prompt, rating in sorted(prompt_ratings.items(), key=lambda item: item[1], reverse=True):
        table.add_row([prompt, rating])

    print(table)


# Main execution
if __name__ == "__main__":
    description = """
    RO-SCIRAW is an advanced prompt engineering framework created by Kirk Lin for optimizing interactions with Large Language Models (LLMs). The framework consists of 8 core components:

    1. R - Role: Define the AI's professional identity and operational mode. [Required]
    2. O - Objectives: Clearly state the specific tasks expected of the AI. [Required]
    3. S - Style: Specify the expression style and tone of the output. [Optional]
    4. C - Content/Context: Provide necessary background information and knowledge base. [Required]
    5. I - Input: Detail the type and format of data provided to the AI. [Required]
    6. R - Response: Specify the expected form and structure of the output. [Required]
    7. A - Audience: Clarify the target group to optimize output relevance. [Optional]
    8. W - Workflow: Provide systematic step-by-step guidelines. [Optional]

    Additionally, it's strongly recommended to provide an example to help the AI understand the expected output.

    Your task is: Given a topic or task, generate a complete RO-SCIRAW framework prompt. Include all required components (Role, Objectives, Content, Input, Response) and optional components (Style, Audience, Workflow). Also provide a relevant example. Ensure the prompt is clear, structured, and fully leverages the advantages of the RO-SCIRAW framework.
    """

    test_cases = [
        {
            'prompt': 'Generate a Chinese introduction of the RO-SCIRAW framework, including its purpose and main components.'},
        {'prompt': 'Explain the core concepts of the RO-SCIRAW framework in simple language.'},
        {'prompt': 'Describe the 8 components of the RO-SCIRAW framework and their functions.'},
        {'prompt': 'Summarize the importance of the RO-SCIRAW framework in prompt engineering.'}
    ]

    generate_optimal_prompt(description, test_cases, NUMBER_OF_PROMPTS)
