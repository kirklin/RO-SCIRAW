from openai import OpenAI
from prettytable import PrettyTable
from tqdm import tqdm
import itertools
from tenacity import retry, stop_after_attempt, wait_exponential

# 配置
OPENAI_API_KEY = "your-api-key-here"  # 输入你的OpenAI API密钥

# 常量
K = 32  # ELO评分系统的K因子
CANDIDATE_MODEL = 'gpt-4o'  # 用于生成候选提示的模型
CANDIDATE_MODEL_TEMPERATURE = 0.9  # 候选模型的温度参数
GENERATION_MODEL = 'gpt-4o-mini'  # 用于生成响应的模型
GENERATION_MODEL_TEMPERATURE = 0.8  # 生成模型的温度参数
GENERATION_MODEL_MAX_TOKENS = 60  # 生成模型的最大token数
N_RETRIES = 3  # 重试次数
RANKING_MODEL = 'gpt-4o-mini'  # 用于排名的模型
RANKING_MODEL_TEMPERATURE = 0.5  # 排名模型的温度参数
NUMBER_OF_PROMPTS = 10  # 要生成的提示数量

client = OpenAI(api_key=OPENAI_API_KEY)

system_gen_system_prompt = """Your job is to generate system prompts for GPT-4, given a description of the use-case and some test cases.

The prompts you will be generating will be for freeform tasks, such as generating a landing page headline, an intro paragraph, solving a math problem, etc.

In your generated prompt, you should describe how the AI should behave in plain English. Include what it will see, and what it's allowed to output. Be creative with prompts to get the best possible results. The AI knows it's an AI -- you don't need to tell it this.

You will be graded based on the performance of your prompt... but don't cheat! You cannot include specifics about the test cases in your prompt. Any prompts with examples will be disqualified.

Most importantly, output NOTHING but the prompt. Do not include anything else in your message."""

ranking_system_prompt = """Your job is to rank the quality of two outputs generated by different prompts. The prompts are used to generate a response for a given task.

You will be provided with the task description, the test prompt, and two generations - one for each system prompt.

Rank the generations in order of quality. If Generation A is better, respond with 'A'. If Generation B is better, respond with 'B'.

Remember, to be considered 'better', a generation must not just be good, it must be noticeably superior to the other.

Also, keep in mind that you are a very harsh critic. Only rank a generation as better if it truly impresses you more than the other.

Respond with your ranking, and nothing else. Be fair and unbiased in your judgement."""



def generate_candidate_prompts(description, test_cases, number_of_prompts):
    """生成候选提示"""
    outputs = client.chat.completions.create(
        model=CANDIDATE_MODEL,
        messages=[
            {"role": "system", "content": system_gen_system_prompt},
            {"role": "user", "content": f"Here are some test cases:`{test_cases}`\n\nHere is the description of the use-case: `{description.strip()}`\n\nRespond with your prompt, and nothing else. Be creative."}
        ],
        temperature=CANDIDATE_MODEL_TEMPERATURE,
        n=number_of_prompts
    )
    prompts = [i.message.content for i in outputs.choices]
    return prompts


def expected_score(r1, r2):
    """计算预期分数"""
    return 1 / (1 + 10 ** ((r2 - r1) / 400))


def update_elo(r1, r2, score1):
    """更新ELO评分"""
    e1 = expected_score(r1, r2)
    e2 = expected_score(r2, r1)
    return r1 + K * (score1 - e1), r2 + K * ((1 - score1) - e2)


@retry(stop=stop_after_attempt(N_RETRIES), wait=wait_exponential(multiplier=1, min=4, max=70))
def get_score(description, test_case, pos1, pos2, ranking_model_name, ranking_model_temperature):
    """获取生成结果的分数"""
    score = client.chat.completions.create(
        model=ranking_model_name,
        messages=[
            {"role": "system", "content": ranking_system_prompt},
            {"role": "user", "content": f"""任务: {description.strip()}
提示: {test_case['prompt']}
生成A: {pos1}
生成B: {pos2}"""}
        ],
        logit_bias={
            '32': 100,  # 'A' token
            '33': 100,  # 'B' token
        },
        max_tokens=1,
        temperature=ranking_model_temperature,
    ).choices[0].message.content
    return score


@retry(stop=stop_after_attempt(N_RETRIES), wait=wait_exponential(multiplier=1, min=4, max=70))
def get_generation(prompt, test_case):
    """获取生成的响应"""
    generation = client.chat.completions.create(
        model=GENERATION_MODEL,
        messages=[
            {"role": "system", "content": prompt},
            {"role": "user", "content": f"{test_case['prompt']}"}
        ],
        max_tokens=GENERATION_MODEL_MAX_TOKENS,
        temperature=GENERATION_MODEL_TEMPERATURE,
    ).choices[0].message.content
    return generation


def test_candidate_prompts(test_cases, description, prompts):
    """测试候选提示"""
    prompt_ratings = {prompt: 1200 for prompt in prompts}
    total_rounds = len(test_cases) * len(prompts) * (len(prompts) - 1) // 2
    pbar = tqdm(total=total_rounds, ncols=70)

    for prompt1, prompt2 in itertools.combinations(prompts, 2):
        for test_case in test_cases:
            pbar.update()
            generation1 = get_generation(prompt1, test_case)
            generation2 = get_generation(prompt2, test_case)
            score1 = get_score(description, test_case, generation1, generation2, RANKING_MODEL,
                               RANKING_MODEL_TEMPERATURE)
            score2 = get_score(description, test_case, generation2, generation1, RANKING_MODEL,
                               RANKING_MODEL_TEMPERATURE)
            score1 = 1 if score1 == 'A' else 0 if score1 == 'B' else 0.5
            score2 = 1 if score2 == 'B' else 0 if score2 == 'A' else 0.5
            score = (score1 + score2) / 2
            r1, r2 = prompt_ratings[prompt1], prompt_ratings[prompt2]
            r1, r2 = update_elo(r1, r2, score)
            prompt_ratings[prompt1], prompt_ratings[prompt2] = r1, r2

            if score > 0.5:
                print(f"胜者: {prompt1}")
            elif score < 0.5:
                print(f"胜者: {prompt2}")
            else:
                print("平局")

    pbar.close()
    return prompt_ratings


def generate_optimal_prompt(description, test_cases, number_of_prompts=10):
    """生成最优提示"""
    prompts = generate_candidate_prompts(description, test_cases, number_of_prompts)
    prompt_ratings = test_candidate_prompts(test_cases, description, prompts)

    table = PrettyTable()
    table.field_names = ["提示", "评分"]
    for prompt, rating in sorted(prompt_ratings.items(), key=lambda item: item[1], reverse=True):
        table.add_row([prompt, rating])

    print(table)


# 主执行
if __name__ == "__main__":
    description = """
    RO-SCIRAW是由Kirk Lin开创的先进提示词工程框架,用于优化与大型语言模型(LLMs)的交互。该框架由8个核心组件构成:
    
    1. R - Role (角色): 定义AI的专业身份和操作模式。[必选]
    2. O - Objectives (目标): 明确指出期望AI完成的具体任务。[必选]
    3. S - Style (风格): 规定输出的表达方式和语气。[可选]
    4. C - Content (内容/上下文): 提供必要的背景信息和知识基础。[必选]
    5. I - Input (输入): 详细说明提供给AI的数据类型和格式。[必选]
    6. R - Response (响应): 规定期望的输出形式和结构。[必选]
    7. A - Audience (受众): 明确目标群体,以优化输出的相关性。[可选]
    8. W - Workflow (工作流): 提供系统化的步骤指南。[可选]

    此外,强烈建议提供一个示例来帮助AI理解预期输出。
    """

    test_cases = [
        {'prompt': '生成 RO-SCIRAW 框架的中文简介，包括其目的和主要组成部分。'},
        {'prompt': '用通俗易懂的语言解释 RO-SCIRAW 框架的核心概念。'},
        {'prompt': '描述 RO-SCIRAW 框架的 8 个组成部分及其作用。'},
        {'prompt': '总结 RO-SCIRAW 框架在提示词工程中的重要性。'}
    ]

    generate_optimal_prompt(description, test_cases, NUMBER_OF_PROMPTS)
